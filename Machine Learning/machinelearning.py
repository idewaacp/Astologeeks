# -*- coding: utf-8 -*-
"""MachineLearning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lo_j8LLpQnqQb11rh3WX9oUW0ZFtCa8j
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

from datetime import datetime
from dateutil.relativedelta import relativedelta

df = pd.read_csv('horoscope_saved.csv')
df = df.drop(columns='date')
#df['date'] = pd.to_datetime(df['date'], format='%Y%m%d')
df

#df['date'] = df['date'].dt.date
#df

df.info()

df.describe(include = 'all')

df.isnull().sum()

df.shape

"""**Visualisasi Data**"""

#Univariated analysis sign

f = plt.figure(figsize=(42,12))
f.add_subplot(1,2,1)
sns.countplot(df['sign'])
plt.show()

#Univariated analysis category

f = plt.figure(figsize=(24,12))
f.add_subplot(1,2,1)
sns.countplot(df['category'])
plt.show()

"""**Data Preperation**"""

# convert to lowercase
df['horoscope'] = df['horoscope'].str.lower()

#from nltk.corpus import stopwords #comment jika Error dan gunakan 2 sintaks dibawah
import nltk
nltk.download('stopwords')

from nltk.corpus import stopwords
stop = set(stopwords.words('english'))
df['horoscope_stopword'] = df['horoscope'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))

df.head()

#label encoding

# Import label encoder 
from sklearn import preprocessing
label_encoder = preprocessing.LabelEncoder() 
df['sign']= label_encoder.fit_transform(df['sign']) 
df['category']= label_encoder.fit_transform(df['category'])

df

#one-hot-encoding

sign = pd.get_dummies(df.sign)
category = pd.get_dummies(df.category)
new_df = pd.concat([df, sign, category], axis=1) #, category
new_df = new_df.drop(columns='sign')
new_df = new_df.drop(columns='category')
new_df

#one-hot-encoding 2

sign = pd.get_dummies(df.sign)
category = pd.get_dummies(df.category)
new_df = pd.concat([df, sign], axis=1) #, category
new_df = new_df.drop(columns='sign')
new_df

horo = new_df['horoscope'].values
labels = new_df[['aquarius', 'aries', 'cancer', 'capricorn', 'gemini', 'leo', 'libra', 'pisces', 'sagittarius', 'scorpio', 'taurus', 'virgo', 'birthday', 'career', 'general', 'love', 'wellness']].values #, 'birthday', 'career', 'general', 'love', 'wellness'

horo = new_df['horoscope'].values
labels = new_df[['aquarius', 'aries', 'cancer', 'capricorn', 'gemini', 'leo', 'libra', 'pisces', 'sagittarius', 'scorpio', 'taurus', 'virgo']].values #, 'birthday', 'career', 'general', 'love', 'wellness'

horo = df['horoscope_stopword'].values
labels = df[['sign', 'category']].values

#memisahkan data training dan validasi (data validasi hanya terdiri 20% dari total dataset)
horo_latih, horo_test, labels_latih, labels_test = train_test_split(horo, labels, test_size=0.2)

#fungsi tokenizer
#filt = '!"#$%&()*+.,-/:;=?@[\]^_`{|}~ '

tokenizer = Tokenizer(num_words=5000, oov_token='x')
tokenizer.fit_on_texts(horo_latih)
tokenizer.fit_on_texts(horo_test)
 
sekuens_latih = tokenizer.texts_to_sequences(horo_latih)
sekuens_test = tokenizer.texts_to_sequences(horo_test)
 
padded_latih = pad_sequences(sekuens_latih) 
padded_test = pad_sequences(sekuens_test)

"""**Modeling**"""

#implementasi callback

class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy')>0.9 and 
       logs.get('val_accuracy')>0.9):
      print("\nAkurasi telah mencapai >90%!")
      self.model.stop_training = True
callbacks = myCallback()

#Arsitektur model machine learning dengan menggunakan embedding dan LSTM
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=13000, output_dim=32),
    tf.keras.layers.LSTM(128),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(2, activation='softmax')
])
model.summary()

#compiler
model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])

num_epochs = 10
history = model.fit(padded_latih, labels_latih, epochs=num_epochs, 
                    validation_data=(padded_test, labels_test), verbose=2, callbacks=[callbacks])

model.save("machinelearning.h5")

"""Save Model to TFjs"""

!pip install tensorflowjs

saved_model_path = '/content/mymodel/'
tf.saved_model.save(model, saved_model_path)

!tensorflowjs_converter \
  --input_format=tf_saved_model \
  /content/mymodel/ \
  /content/modeltfjs